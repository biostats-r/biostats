---
output:
  html_document:
    css: custom-blockquote.css
--- 

```{r setup&variables, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# title: Pairwise Comparisons with corrections
# author: jonathan
# revised: 31/1/2020 

WEB <- c("https://bioceed.uib.no/dropfolder/bioSTATS/Rmd/Basic/post-hoc tests/pairwise.html")
GIT <- c("https://github.com/jonathansoule/biostats/blob/master/Basic/post-hoc tests/pairwise.Rmd")
```

The function `pairwise.t.test()` allows for performing **multiple t-tests** on a multitude of groups using **all possible pairwise comparisons**. This comes handy as a follow up to [one-way ANOVA](https://biostats.w.uib.no/one-way-anova/){target="_blank"}, [factorial ANOVA](https://biostats.w.uib.no/factorial-design/){target="_blank"}, [Kruskal-Wallis test](https://biostats.w.uib.no/comparing-two-means-kruskal-wallis-test/){target="_blank"} with more than 2 groups,... In addition, most of these pairwise t-tests that this function can run assume **normality of distribution and equality of variances** which you needed anyway to perform ANOVA.

Here is a short list of the tests that `pairwise.t.test()` can run (extracted from the documentation of `p.adjust`):

> The adjustment methods include the Bonferroni correction ("bonferroni") in which the p-values are multiplied by the number of comparisons. Less conservative corrections are also included by Holm (1979) ("holm"), Hochberg (1988) ("hochberg"), Hommel (1988) ("hommel"), Benjamini & Hochberg (1995) ("BH" or its alias "fdr"), and Benjamini & Yekutieli (2001) ("BY"), respectively. 

Here we will reuse the example introduced [here (one-way ANOVA)](https://biostats.w.uib.no/one-way-anova/){target="_blank"}.
To create the corresponding dataframe, use the following code:
```{r dataframe}
# response variable
size <- c(25,22,28,24,26,24,22,21,23,25,26,30,25,24,21,27,28,23,25,24,20,22,24,23,22,24,20,19,21,22)

# predictor variable
location <- as.factor(c(rep("ForestA",10), rep("ForestB",10), rep("ForestC",10)))

# dataframe
my.dataframe <- data.frame(size,location)
```
<br/><br/>

### Assumptions

The assumptions for most of the tests are:

+ the observations are **independent**,
+ **normality** of distribution,
+ **homogeneity of variance**.

<br/><br/>

### Running the test

This syntax is `pairwise.t.test(response, predictor, p.adjust.method= )` where `response` is the response variable and `predictor `is the predictor variable. In addition, `pairwise.t.test()` allows you to apply the method of your choice when it comes to correcting/adjusting p-values to control type I errors. Simply add the name or abbreviation for the method to the argument `p.adjust.method=`. The methods are `none` (no correction), `bonferroni`, `holm`, `hochberg`, `hommel`, `BH` and `BY`. Choosing the right correction for your analysis depends a lot on your experimental design.

Now letâ€™s look at 3 cases of post hoc pairwise t-tests, the first one without correction, the second one with bonferroni correction and the last one with hochberg correction:

```{r pairwise}
pairwise.t.test(size, location, p.adjust.method="none")
pairwise.t.test(size, location, p.adjust.method="bonferroni")
pairwise.t.test(size, location, p.adjust.method="hochberg")
```

The three outputs are presented above. Each one displays the same type of table with pairwise comparisons, the last line shows the adjustment method. The results are very different in these 3 cases. Using no correction or hochberg correction, 2 p-values drop under 0.05 (yellow and blue boxes), whereas only one p-value is less than 0.05 when using bonferroni correction. This difference may be decisive for interpreting the outcome of your experiment and may change a lot the conclusions of your research. Therefore, it is important that you weigh the pros and cons when deciding which adjustment method to apply to your dataset.

***
<!-- UNCOMMENT TO ACTIVATE
Get the Rmarkdown file corresponding to this page on [GitHub](`r GIT`){target="_blank"}.
-->
