[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Enough targets to Write a Thesis",
    "section": "",
    "text": "1 Introduction\nWriting a thesis requires importing data (1), writing code to clean, transform (2), analyse data (3), and making figures (5) and writing the thesis (6; FigureÂ 1.1). This is done by writing several R scripts and running one script after another producing results and figures. All the time, the code is updated, to add new data, transforming the data, changing analysis or adding a figure. Often several iterations of running the same scripts are needed and it is difficult to keep track of which scripts need rerunning. In addition, complex and computational heavy data analysis can take a lot of time especially when it requires rerunning the analysis.\nThis workflow is very inefficient, non-reproducible and error prone. And there is a better way.\n\n\n\n\n\nFigureÂ 1.1: Non-reproducible data workflow.\n\n\n\n\nTargets is a pipeline tool which takes care of dependencies in the code and keeps track of outdated objects. A target pipeline can for example be data import, analysis, making a figure and produce a report (1; (FigureÂ 1.2). When updating and rerunning one part of the code, targets will skip parts where the upstream code has not changed and are therefore still up to date (2). Targets will only rerun the code that is outdated and ensures that your results always match the underlying code and maintains a reproducible workflow (3). It avoids unnecessary repetition and can saves costly running time.\n\n\n\n\n\nFigureÂ 1.2: Reproducible targets pipeline.\n\n\n\n\nWhen is targets useful?\n\nWhen the code has a long runtime because it is slow or complex\nWhen the workflow has interconnected tasks with dependencies\n\n\n\n\n\n\n\nBefore you start\n\n\n\nUse the checker function to check what software and R packages you have already installed and what needs updating or installing.\nIf you have never used the package before run:\nmaybe first: install.packages(\"remotes\")\nthen remotes::install_github(\"richardjtelford/checker\")\n\nTo check what you have already installed use: chk_requirements(...)\nFor this tutorial you will need:\n\nRStudio (version 2022.2 or newer)\nR\nquarto\nquarto R package (install.packages(\"quarto\"))\ntargets R package (install.packages(\"targets\"))\ntarchetypes R package (install.packages(\"tarchetypes\"))\n\nIf quarto is new to you, have a look at the Reproducible documents tutorial (REF to Quarto book) before proceeding here."
  },
  {
    "objectID": "02-workflow.html#the-targets-pipeline",
    "href": "02-workflow.html#the-targets-pipeline",
    "title": "\n2Â  Getting started with targets\n",
    "section": "\n2.1 The targets pipeline",
    "text": "2.1 The targets pipeline\nA target workflow has a specific file structure including R code, functions, qmd files, data and a _targets.R file (FigureÂ 2.1). The _targets.R file is mandatory and the most important file defining the targets pipeline. This file lives at the root of the R project folder.\nAn R project has many other files and it is recommended to keep code and data files in separate folders to keep the repository tidy. It is common to have one or several scripts that contain custom user-defined functions (R/functions.R). Targets pipelines are based on functions, which is good practice coding (e.g.Â avoid repetition), and also keeps the pipeline tidy.\n\n\n\n\nFigureÂ 2.1: File structure of an R Studio project with a target pipeline.\n\n\n\n\nTo set up this file structure use the use_targets() function, which creates an initial _targets.R script with comments to help you populate the script.\nNote that it also creates a couple of other files, one of which is called run.R. This is a helper script to run the pipeline and will be explained later.\n\n\n\n\n\n\nExercise\n\n\n\nGo to the Svalbard trait project and start to set up a targets pipeline by using the use_targets() function.\n\n\n\n2.1.1 _target.R script file\nThe _targets.R file configures and defines the pipeline. This file is mandatory and without it the targets pipeline will not work. When using the use_targets() function, it sets up the basic structure and comments to help fill out the rest (see below).\nThe _targets.R file contains the following components:\n\nSet options such as load necessary packages or defining the output format using tar_option_set().\nRun R scripts containing custom functions using the tar_source() function.\nMake a list of targets, which define the pipeline and is created with tar_target(). Each target is a step in the workflow, for example importing data, analysis or figure and looks like a normal R object (e.g.Â tibble, vector, figure). The targets are stored in _targets/objects/ and can be used downstream in the pipeline. One advantage of the targets pipeline is that once the pipeline has run, all targets can be reproduced using tar_load() and the pipeline does not need rerunning each time before accessing the targets. To access all the targets at once. use tar_load_everything(). In tar_load() you can also use tidy select commands to load specific targets, e.g.Â tar_load(starts_with(\"y\"))\n\nNote that the file also contains other options which are optional.\n\n# Created by use_targets().\n# Follow the comments below to fill in this target script.\n# Then follow the manual to check and run the pipeline:\n#   https://books.ropensci.org/targets/walkthrough.html#inspect-the-pipeline # nolint\n\n# Load packages required to define the pipeline:\nlibrary(targets)\n# library(tarchetypes) # Load other packages as needed. # nolint\n\n# Set target options:\ntar_option_set(\n  packages = c(\"tibble\"), # packages that your targets need to run\n  format = \"rds\" # default storage format\n  # Set other options as needed.\n)\n\n# tar_make_clustermq() configuration (okay to leave alone):\noptions(clustermq.scheduler = \"multicore\")\n\n# tar_make_future() configuration (okay to leave alone):\n# Install packages {{future}}, {{future.callr}}, and {{future.batchtools}} to allow use_targets() to configure tar_make_future() options.\n\n# Run the R scripts in the R/ folder with your custom functions:\ntar_source()\n# source(\"other_functions.R\") # Source other scripts as needed. # nolint\n\n# Replace the target list below with your own:\nlist(\n  tar_target(\n    name = data,\n    command = tibble(x = rnorm(100), y = rnorm(100))\n#   format = \"feather\" # efficient storage of large data frames # nolint\n  ),\n  tar_target(\n    name = model,\n    command = coefficients(lm(y ~ x, data = data))\n  )\n)\n\nNow it is your turn to populate the targets pipeline.\n\n2.1.2 Make custom functions\nTargets workflows are based on functions. Functions are very useful if a task has to be done multiple times.\nFunctions are made with the keyword function(), can have one or more arguments separated by commas, and need assigning to a name (e.g.Â my_function). Let us make an example we multiply two numbers, but the numbers are not always the same. This is a case for a function.\n\nmy_function <- function(arg1, arg2){\n  arg1 * arg2\n}\n\nmy_function(arg1 = 3, arg2 = 4)\n\nFunctions in the targets workflow need to be saved in a script in the folder R/. The _target.R script will look for all R files in this folder and run them.\nHere is a working example for a function that runs a linear regression:\n\nfit_model <- function(data){\n  mod <- lm(Value ~ Treatment, data = data)\n  mod\n}\n\nfit_model(data = my_data)\n\n\n\n\n\n\n\nExercise\n\n\n\nGo to the Svalbard trait project create a file called functions.R and save it in the R/ folder. Then add three functions that do the fallowing:\n\nimport the data and filter the species Bistorta vivipara and the trait plant height\nfit a linear regression to test how the warming treatment affects plant height\nand make a boxplot showing plant height in control and warming treatments.\n\nThe code you need for this exercise is already in the svalbard_trait.qmd file. You need to copy it and turn it into functions.\nWhen you are finished, test if the functions are doing what you think they do, i.e.Â test them with a small dataset.\n\n\nThe next step is to configure and define the targets pipeline.\n\n2.1.3 Populate the _targets.R file\nThe _targets.R file has 3 main components that need to be populated.\n\nUse tar_option_set() and the argument package to load all the required packages that are needed to run the pipeline. Note that targets and tarchetypes need to be loaded first and outside this function, otherwise the pipeline will not work. Functions that are only used in a quarto file can be loaded directly in there and do not need to be loaded here. The argument format letâ€™s you define default storage format.\n\n\n# Load packages required to define the pipeline:\nlibrary(targets)\nlibrary(tarchetypes) # Load other packages as needed.\n\n# Set target options:\ntar_option_set(\n  packages = c(\"tibble\"), # packages that your targets need to run\n  format = \"rds\" # default storage format\n  # Set other options as needed.\n)\n\n\nThe function tar_source() will source all the R scripts in the R/ folder. You do not need to do anything else here. If there are scripts that you do not want to run at the moment, move them to another place.\nThe pipeline is a list of target objects, separated by commas. Each target is declared by the tar_target() function. This function needs the argument name and command that defines the code to produce the target. Here is an example of a target that uses the function to run a regression from above.\n\n\n# fit model for plant height\nlist(\n  tar_target(name = mod_height,\n             command = fit_model(data))\n  )\n\nTarget names should be unique (no duplicates), should not start with a dot and the name should be meaningful (do not use my_variable).\nData files are special, because they also need the argument format to decalre that this is a file. Targets will check if the file has been changed and automatically import the data again the next time the pipeline is run.\n\nlist(\n  tar_target(name = file,\n             command = \"data/PFTC4_Svalbard_2018_ITEX_Traits.csv\",\n             format = \"file\")\n  )\n\nA target usually creates a dataset, analyse data using a model or summarise or vizualise a dataset. A target should do one thing only and if a functions gets too long, it can be split into nested sub-functions to make the code readable and easier to maintain. You also want to keep the number of targets manageble. Do not put too much in one target, and also try not to procude too many.\n\n\n\n\n\n\nExercise\n\n\n\nPopulate the _targets.R file:\n\nadd all packages to the tar_option_set() function in the _targets.R file that are needed to run the pipeline.\ncheck that your _targets.R file sources the custom functions tar_source() and that all your functions are in the right place.\nand set up the pipeline with three targets that imports the data, runs the model and makes a figure.\n\n\n\n\n2.1.4 Output files\n\nThe results and figures of an analysis are usually presented in a document or presentation. An output files, such as a reproducible quarto document can be added to a targets pipeline. The targets that have been produced in the pipeline can be used in the quarto file, for example a figure can be plotted.\nThe targets that are used in the quarto document need to be loaded into the current environment. For this we can use tar_load() or tar_read(). The first function is used when a target is used several times. tar_read() is useful if a target is only needed once, e.g.Â to show a figure.  Once the target is loaded, it can be printed.\n\n# print model output\ntar_read(mod_height)\ntidy(mod_height)\n\nTo add the quarto document to the pipeline, the manuscript has to be rendered. This is done in _targets.R file in the list of targets using tar_quarto().\n\n# render ms\ntar_quarto(name = manuscript, path = \"svalbard_traits.qmd\")\n\nNote that all packages that are needed to run the quarto file need to be loaded in the .qmd file. If you are using a function exclusively in the quarto script, the package can be loaded only in the quarto file, but not in the _target.R file. targets and tarchetypes always need to be loaded in both files.\n\n\n\n\n\n\nExercise\n\n\n\nPrepare your svalbard_traits.qmd script and add it to the pipeline.\n\nLoad all the necessary R packages in the quarto file.\nLoad all the necessary targets that you want to use in the quarto file.\nAnd add the quarto file to the pipeline using tar_quarto().\nClean the quarto file and remove the unnecessary code, which has been moved to the custom functions.\n\n\n\n\n2.1.5 Inspect and run the pipeline\nWe are now ready to inspect the pipeline, check for errors and run it. Use tar_manifest() to check for errors. This function lists useful information about each target, let you know if you are missing a R package and check for missing or duplicate targets (FigureÂ 2.2).\n\n\n\n\nFigureÂ 2.2: Names and commands for each target in the pipeline.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRun the tar_manifest() function to check if the pipeline is properly set up.\n\n\nNow we are ready to run the pipeline. For this open the run.R script and run the tar_make() function. This function looks for the _targets.R in the working directory and runs the pipeline.\nWhen running the pipeline for the first time you will see a list of all the targets and each of them is built. Once you have run the pipeline, it will always skip the targets that have not changed and are up to date and only run the once that need updating (FigureÂ 2.3). In the long run this will save a lot of computational time and is one of the big advantages of using targets pipelines.\n\n\n\n\nFigureÂ 2.3: R output after running the pipeline.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nOpen the run.R script and run the pipeline. Hopefully, everything will run smoothly ðŸ¤ž! If not check out the Trouble shooting section below.\nRun the pipeline again and check if the targets that are already built are skipped.\nChange something in your pipeline and run it again and see what happens. You can for example add a sentence in the quarto file, or change the colour in the figure.\n\n\n\n\n\n\n\n\nDo it step by step\n\n\n\nTarget plans can be huge and complex. Start small, create a few targets and functions and make the plan running. Then add new code in small steps and check regularly if the plan is still working This will help to understand and solve errors (see next section).\n\n\n\n\nFigureÂ 2.4: Vizualisation of complex target pipeline."
  },
  {
    "objectID": "02-workflow.html#trouble-shooting",
    "href": "02-workflow.html#trouble-shooting",
    "title": "\n2Â  Getting started with targets\n",
    "section": "\n2.2 Trouble shooting",
    "text": "2.2 Trouble shooting\n\n2.2.1 Vizualise the pipeline\nIf something goes wrong, a good place to start is to vizualise your pipeline. The tar_visnetwork() function shows the dependency graph of the pipeline. Circles are targets, triangles functions, and the colour indicates if the targets are up to date or not.\n\n\n\n\nFigureÂ 2.5: Vizualise the targets pipeline.\n\n\n\n\n\n2.2.2 Object not found\nA common error is to call a target that does not exist. When running the pipeline this error will appear (FigureÂ 2.6). This is usually if the name is spelled wrong or when using an old name.\n\n\n\n\nFigureÂ 2.6: Error message for missing object\n\n\n\n\n\n2.2.3 Duplicate target\nAnother common mistake is to use the same name for two different targets (FigureÂ 2.7). This is common when copy pasting code. Rename one of the objects and the problem is solved.\n\n\n\n\nFigureÂ 2.7: Error message for duplicate target.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nIf you have problems getting the target pipeline running, here is a working example that you can download or check online. Note that you need to have all the packages that are required for the targets pipeline to run installed.\n\n#install.packages(\"usethis\") # if you don't have it already.\nusethis::use_course(\"biostats-r/targets_workflow_svalbard\")"
  },
  {
    "objectID": "02-workflow.html#resources",
    "href": "02-workflow.html#resources",
    "title": "\n2Â  Getting started with targets\n",
    "section": "\n2.3 Resources",
    "text": "2.3 Resources\n\nThe target manual contains everything you need to know \n\nHere is a large and working target plan\n\nHere is a short introduction video"
  },
  {
    "objectID": "02-encapsulated-code.html",
    "href": "02-encapsulated-code.html",
    "title": "2Â  Encapsulated code",
    "section": "",
    "text": "To understand why targets is useful, we need to introduce the concept of encapsulated code.\nSome sentences about why this is important. Targets workflows are based on functions. Functions are very useful if a task has to be done multiple times.\n\n2.0.1 Make custom functions\nLetâ€™s say we want to multiplies two numbers, but the numbers are not always the same. This is a case to use a function.\nFunctions are made with the keyword function(), can have one or more arguments separated by commas, and need assigning to a name.\nFor a function that multiplies two numbers, we need two arguments, arg1 and arg2. We will give the function a name. Itâ€™s useful to give the function a name that has a meaning and not to use my_function.\n\nmy_multiplier <- function(arg1, arg2){\n  arg1 * arg2\n}\n\nTo run the function, we type the name of the function and define the values for each argument. The function will then return the result.\n\nmy_multiplier(arg1 = 3, arg2 = 4)\n\n[1] 12\n\n\nWe can also make more complicated functions, for example one that runs a linear regression. The function has one argument, which is the data. The function contains a linear regression with value as response and Treatment as predictor. We name the function fit_model.\nTo run the function we type the name of the function and define which data that should be used.\n\nfit_model <- function(data){\n  mod <- lm(Value ~ Treatment, data = data)\n  mod\n}\n\nfit_model(data = my_data)"
  },
  {
    "objectID": "03-workflow.html#the-targets-pipeline",
    "href": "03-workflow.html#the-targets-pipeline",
    "title": "3Â  Getting started with targets",
    "section": "3.2 The targets pipeline",
    "text": "3.2 The targets pipeline\n\n3.2.1 The file structure\nA target workflow has a specific file structure including R code, functions, qmd files, data and a _targets.R file (FigureÂ 3.2). The _targets.R file is mandatory and the most important file defining the targets pipeline. This file lives at the root of the R project folder.\n\n\n\n\n\nFigureÂ 3.2: File structure of an R Studio project with a target pipeline.\n\n\n\n\nAn R project has many other files and it is recommended to keep code and data files in separate folders to keep the repository tidy. It is common to have one or several scripts that contain custom user-defined functions. These scripts should be stored in one folder. In this example we will call the folder R: R/functions.R.\nTo set up this file structure in an RStudio project, use the use_targets() function, which creates an initial _targets.R script with comments to help you populate the script. Note that it also creates a couple of other files, one of which is called run.R. This is a helper script to run the pipeline and will be explained later.\n\n\n\n\n\n\nExercise\n\n\n\nGo to the Svalbard trait project and load the targets library library(targets). Then start to set up a targets pipeline by using the use_targets() function.\n\n\n\n\n3.2.2 The _target.R file\nThe _targets.R file is the main script and configures and defines the pipeline. This file is mandatory and without it the targets pipeline will not work. When using the use_targets() function, it sets up the basic structure and comments to help fill out the rest (see below).\n\n# Created by use_targets().\n# Follow the comments below to fill in this target script.\n# Then follow the manual to check and run the pipeline:\n#   https://books.ropensci.org/targets/walkthrough.html#inspect-the-pipeline # nolint\n\n# Load packages required to define the pipeline:\nlibrary(targets)\n# library(tarchetypes) # Load other packages as needed. # nolint\n\n# Set target options:\ntar_option_set(\n  packages = c(\"tibble\"), # packages that your targets need to run\n  format = \"rds\" # default storage format\n  # Set other options as needed.\n)\n\n# tar_make_clustermq() configuration (okay to leave alone):\noptions(clustermq.scheduler = \"multicore\")\n\n# tar_make_future() configuration (okay to leave alone):\n# Install packages {{future}}, {{future.callr}}, and {{future.batchtools}} to allow use_targets() to configure tar_make_future() options.\n\n# Run the R scripts in the R/ folder with your custom functions:\ntar_source()\n# source(\"other_functions.R\") # Source other scripts as needed. # nolint\n\n# Replace the target list below with your own:\nlist(\n  tar_target(\n    name = data,\n    command = tibble(x = rnorm(100), y = rnorm(100))\n#   format = \"feather\" # efficient storage of large data frames # nolint\n  ),\n  tar_target(\n    name = model,\n    command = coefficients(lm(y ~ x, data = data))\n  )\n)\n\nHave a look at your _targets.R file.\n\n\n\n\n\n\nExercise\n\n\n\nOpen the _targets.R in your repo and have a look at your _targets.R file.\n\n\nThe _targets.R file has three main components. Note that the file also contains other options which are optional. Letâ€™s go through each component step by step.\n\ntar_option_set() sets all options such as load necessary packages or defining the output format. The argument package should have a list of all the required packages that are needed to run the pipeline. Note that targets and tarchetypes need to be loaded first and outside this function, otherwise the pipeline will not work. If your pipeline includes a quarto file, the packages that are only used in a quarto file can be loaded directly in there and do not need to be loaded in the _targets.R file. The argument format letâ€™s you define default storage format.\n\n\n# Load packages required to define the pipeline:\nlibrary(targets)\nlibrary(tarchetypes) # Load other packages as needed.\n\n# Set target options:\ntar_option_set(\n  packages = c(\"tibble\"), # packages that your targets need to run\n  format = \"rds\" # default storage format\n  # Set other options as needed.\n)\n\n\nThe function tar_source() will source all the R scripts in the R/ folder. \n\n\n# Run the R scripts in the R/ folder with your custom functions:\ntar_source()\n# source(\"other_functions.R\") # Source other scripts as needed. # nolint\n\n\nThe last section makes a list of targets which is the pipeline. Each target is a step in the pipeline, for example importing data, run an analysis or make a figure and looks like a normal R object (e.g.Â tibble, vector, figure). Each target is declared by the tar_target() function and separated by a comma. The tar_target() needs two arguments: name defines the target name and command the code to produce the target. This can be a couple of lines of code or calling a function.\n\nHere is a target that uses the function fit_model() that was created in the previous chapter to run a linear regression.\n\n# fit model for plant height\nlist(\n  tar_target(name = model,\n             command = fit_model(data))\n  )\n\nEach target should have a unique name can be called downstream in the pipeline. The pipeline will figure out the order of which step depends on which other steps by iteslf.\nOnce the pipeline has run, the targets are stored in _targets/objects/. All targets can be reproduced using tar_load() and the pipeline does not need rerunning each time before accessing the targets. This is a huge advantage of the targets pipeline and can save a lot of time. To access all the targets at once. use tar_load_everything(). In tar_load() you can also use tidy select commands to load specific targets, e.g.Â tar_load(starts_with(\"y\"))\n\n\n\n\n\n\nTargets names\n\n\n\nTargets names should be unique (no duplicates), should not start with a dot and the name should be meaningful (do not use my_variable).\n\n\nData files are special targets, because they also need the argument format to declare that this target is a file. Each time the pipeline is run, targetes will check if the file has been changed and if this is the case automatically import the data again the next time the pipeline is run.\n\nlist(\n  tar_target(name = file,\n             command = \"data/PFTC4_Svalbard_2018_ITEX_Traits.csv\",\n             format = \"file\")\n  )\n\n\n\n\n\n\n\nHow many targets should I make?\n\n\n\nA target should do one thing only (e.g.Â make a figure) and if a functions gets too long, it can be split into nested sub-functions to make the code readable and easier to maintain. Keep the number of targets manageable, which means keep a balance between the amount of code that goes in one target and the number of targets.\n\n\n\n\n3.2.3 Populate the _target.R file\n\nThe next step is to populating the _targets.R file. We need to set the options, make custom R functions, and define the pipeline.\nLetâ€™s get started.\nFirst, we need to add all R packages to the tar_option_set() function in the _targets.R file that are needed to run the pipeline.\n\n\n\n\n\n\nExercise\n\n\n\nOpen your _targets.R file. Check the first code block in the svalbard_trait.qmd file in your repo to see which R packages that have been used.\nAdd both R packages to tar_option_set(). \n\n# Set target options:\ntar_option_set(\n  packages = c(\"tidyverse\") # packages that your targets need to run\n)\n\n\n\nThe next step is to make a function that runs a linear model.\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new R file called functions.R and save it in a new folder called R.\nMake a function that runs a linear model.\nThe code for this exercise is in the previous chapter.\n\nfit_model <- function(data, response, predictor){\n  mod <- lm(response ~ predictor, data = data)\n  mod\n}\n\n\n\nThe last step is to set up the targets pipeline. Our pipeline should have four steps:\n\ndefine the data file\nimport the data and filter for the desired species and trait\nuse our custom function created above to run a linear model testing the effect of a warming treatment on plant height in Bistorta vivipara\nmake a figure that shows Bistorta vivipara height for control and warming treatment.\n\nLetâ€™s do this step by step.\n\n\n\n\n\n\n\nExercise\n\n\n\nDefine the data file\nOpen the _targets.R file and go to the list of targets.\nUse tar_targets() to define each target. The first argument is name, for example file. The second argument is command, which in our case is the path to the data file. Because this target is a file, we also need a third argument format = \"file\".\nReplace path with the relative path to the data file in your repo.\n\nlist(\n  tar_target(name = file,\n             command = path,\n             format = \"file\")\n  )\n\n\n\nThe next step is to import and filter the data.\n\n\n\n\n\n\n\nExercise\n\n\n\nImport and filter the data\nIn the _targets.R file add a new target using tar_targets(), separated by a comma.\nThis time we can give it the name bistorta to keep it consistent with the svalbard_trait.qmd file. Second for the command we need code to import and filter the data. All the code you need is in the svalbard_trait.qmd.\nNote that in the previous target we have already defined the data file and it is now called file. Because file is a target we can use it directly and do not have to use the path to the data file again.\nAdd code to filter for Bistorta vivipapara and the trait plant height.\n\nlist(\n  tar_target(name = bistorta,\n             command = read_csv(file) |> \n               filter())\n  )\n\n\n\nThe next step in the pipeline is to run the model.\n\n\n\n\n\n\nExercise\n\n\n\nRun model\nAdd a new target in the _targets.R file.\nUse mode_height for the name. The command is the function fit_model() we created previously. The function has three arguments: data, response and predictor.\n\nlist(\n  tar_target(name = mod_height,\n             command = fit_model(data = bistorta, \n                                 response = Value, \n                                 predictor = Treatment))\n  )\n\n\n\nThe final step is to make a figure.\n\n\n\n\n\n\nExercise\n\n\n\nMake figure\nAdd a new target in the _targets.R file.\nUse fig_height for the name. The command is code that produces a figure. We can copy the code from the svalbard_tratis.qmd script.\n\nlist(\n  tar_target(name = fig_height,\n             command = ggplot(bistorta, aes(x = Treatment, y = Value)) +\n               geom_boxplot(fill = c(\"grey80\", \"red\")) +\n               labs(x = \"Treatment\", y = \"Plant height (cm)\"))\n  )\n\n\n\nWell done, you have just set up your first target pipeline. Have a treat ðŸ¥•!\n\n\n\n\n\n\nDo it step by step\n\n\n\nTargets plans can become huge and complex. Start small, create a few targets and functions and make the plan running. Then add new code in small steps and check regularly if the plan is still working This will help to understand and solve errors (see trouble shooting section).\n\n\n\n\n\nFigureÂ 3.3: Vizualisation of complex target pipeline.\n\n\n\n\n\n\n\n\n\n3.2.4 Inspect and run the pipeline\nWe are now ready to inspect the pipeline, check for errors and run it. Use tar_manifest() to check for errors. This function lists useful information about each target, let you know if you are missing a R package and check for missing or duplicate targets (FigureÂ 3.4).\n\n\n\n\n\nFigureÂ 3.4: Names and commands for each target in the pipeline.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRun the tar_manifest() function to check if the pipeline is properly set up. If there are errors fix them.\n\n\nNow we are ready to run the pipeline. For this open the run.R script and run the tar_make() function. This function looks for the _targets.R in the working directory and runs the pipeline.\nWhen running the pipeline for the first time you will see a list of all the targets that are built (FigureÂ 3.5). This is indicated by start target and build target. Once the pipeline has run, it will always skip the targets that have not changed and are up to date and only run the once that need updating. In the long run this will save a lot of computational time and is one of the big advantages of using targets pipelines.\n\n\n\n\n\nFigureÂ 3.5: R output after running the pipeline.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nOpen the run.R script and run the pipeline. Hopefully, everything will run smoothly ðŸ¤ž! If not check out the Trouble shooting section below.\nRun the pipeline again and check if the targets that are already built are skipped.\nChange something in your pipeline and run it again and see what happens. You can for example change the name of a target and see if it is updated when running the pipeline again.\n\n\n\n\n3.2.5 Output files\nThe results and figures of an analysis are usually presented in a report or presentation or both. One or several output files, such as a quarto document can be added to a targets pipeline. The targets that have been produced in the pipeline can be loaded and used in the quarto document, for example a figure.\nTo add the quarto document to the pipeline, the manuscript has to be rendered. This is done in _targets.R file in the list of targets using tar_quarto().\n\n# render ms\ntar_quarto(name = manuscript, path = \"traits_quarto_template.qmd\")\n\n\n\n\n\n\n\nExercise\n\n\n\nWe have prepared a quarto template file. Add this output file traits_quarto_template.qmd to the pipeline in the _targets.R using tar_quarto() as shown above.\n\n\nThe targets that are used in the quarto document need to be loaded into the current environment. For this we can use tar_load() or tar_read(). The first function is used when a target is used several times. tar_read() is useful if a target is only needed once, e.g.Â to show a figure.  Once the target is loaded, it can be printed.\n\n# print model output\ntar_read(mod_height)\ntidy(mod_height)\n\nAll R packages that are needed to run the quarto file need to be loaded in the .qmd file. If you are using a R package exclusively in the quarto script, the package can be loaded only in the quarto file, and does not need to be added to the _target.R file. targets and tarchetypes always need to be loaded in both files.\nThe last step is to prepare the .qmd file, so that it displays the figure.\n\n\n\n\n\n\nExercise\n\n\n\nPrepare the traits_quarto_template.qmd file\nIn this output file, we want to show the figure displaying plant height in control vs.Â warmed plots. For this, you need to load the target for the figure into the environment. Because we will use the figure only once you can use tar_read(fig_height).\n\ntar_read(fig_height)\nfig_height\n\nRender the file with the Render button on top of the script to check if the code is good. If it runs smoothly, run the pipeline again and check the traits_quarto_template.html output file."
  },
  {
    "objectID": "03-workflow.html#trouble-shooting",
    "href": "03-workflow.html#trouble-shooting",
    "title": "3Â  Getting started with targets",
    "section": "3.3 Trouble shooting",
    "text": "3.3 Trouble shooting\n\n3.3.1 Vizualise the pipeline\nIf something goes wrong, a good place to start is to visualize your pipeline. The tar_visnetwork() function shows the dependency graph of the pipeline. Circles are targets, triangles functions, and the colour indicates if the targets are up to date or not.\n\n\n\n\n\nFigureÂ 3.6: Vizualise the targets pipeline.\n\n\n\n\n\n\n3.3.2 Object not found\nA common error is to call a target that does not exist. When running the pipeline this error will appear (FigureÂ 3.7). This is usually if the name is spelled wrong or when using an old name.\n\n\n\n\n\nFigureÂ 3.7: Error message for missing object\n\n\n\n\n\n\n3.3.3 Duplicate target\nAnother common mistake is to use the same name for two different targets (FigureÂ 3.8). This is common when copy pasting code. Rename one of the objects and the problem is solved.\n\n\n\n\n\nFigureÂ 3.8: Error message for duplicate target.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nIf you have problems getting the target pipeline running, here is a working example that you can download or check online. Note that you need to have all the packages that are required for the targets pipeline to run installed.\n\n#install.packages(\"usethis\") # if you don't have it already.\nusethis::use_course(\"biostats-r/targets_workflow_svalbard\")"
  },
  {
    "objectID": "03-workflow.html#resources",
    "href": "03-workflow.html#resources",
    "title": "3Â  Getting started with targets",
    "section": "3.4 Resources",
    "text": "3.4 Resources\n\nThe target manual contains everything you need to know \nHere is a large and working target plan\nHere is a short introduction video"
  },
  {
    "objectID": "03-workflow.html#introduction-to-targets",
    "href": "03-workflow.html#introduction-to-targets",
    "title": "3Â  Getting started with targets",
    "section": "3.1 Introduction to targets",
    "text": "3.1 Introduction to targets\nTargets is a pipeline tool, which coordinates the different steps in data science i R. It takes care of dependencies in the code and keeps track of outdated objects.\nA targets pipeline consists of different steps, such as importing data, running an analysis or making a figure (FigureÂ 3.1). Each step in the pipeline is a target and can for example be a data frame, a model or a figure. A target is basically an R object in memory. It is created by a piece of code, often this is a function. A pipeline has a main script that puts all the pieces of code together and takes care of dependencies and keeping track of changes.\nThis concept should sound familiar to you after reading the previous chapter on abstraction.\n\n\n\n\n\nFigureÂ 3.1: Simple targets pipeline.\n\n\n\n\n\n\n\n\n\n\nDefinitions\n\n\n\n\npipeline tool - coordinates different steps of data science\ntarget - an R object in memory\nfunction - self contained modules of code that accomplish a specific task\n\n\n\nLetâ€™s have a look at the targets pipeline."
  },
  {
    "objectID": "02-encapsulated-code.html#make-custom-functions",
    "href": "02-encapsulated-code.html#make-custom-functions",
    "title": "2Â  Abstraction",
    "section": "2.1 Make custom functions",
    "text": "2.1 Make custom functions\nLetâ€™s say we want to multiplies two numbers, but the numbers are not always the same. This is a case to use a function.\nFunctions are made with the keyword function(), can have one or more arguments separated by commas, and need assigning to a name.\nFor a function that multiplies two numbers, we need two arguments, arg1 and arg2. We will give the function a name. Itâ€™s useful to give the function a name that has a meaning and not to use my_function.\n\nmy_multiplier <- function(arg1, arg2){\n  arg1 * arg2\n}\n\nTo run the function, we type the name of the function and define the values for each argument. The function will then return the result.\n\nmy_multiplier(arg1 = 3, arg2 = 4)\n\n[1] 12\n\n\nWe can also make more complicated functions, for example one that runs a linear regression. The function has one argument, which is the data. The function contains a linear regression with value as response and Treatment as predictor. We name the function fit_model.\nTo run the function we type the name of the function and define which data that should be used.\n\nfit_model <- function(data){\n  mod <- lm(Value ~ Treatment, data = data)\n  mod\n}\n\nfit_model(data = my_data)"
  },
  {
    "objectID": "02-encapsulated-code.html#resources",
    "href": "02-encapsulated-code.html#resources",
    "title": "2Â  Abstraction",
    "section": "2.2 Resources",
    "text": "2.2 Resources\n\nFilazzola, A., & Lortie, C. J. (2022). A call for clean code to effectively communicate science. Methods in Ecology and Evolution, 13(10), 2119-2128."
  },
  {
    "objectID": "02-encapsulated-code.html#functions",
    "href": "02-encapsulated-code.html#functions",
    "title": "2Â  Abstraction",
    "section": "2.1 Functions",
    "text": "2.1 Functions\n\n2.1.1 Make custom functions\nLetâ€™s say we want to multiplies two numbers, but the numbers are not always the same. This is a case to use a function.\nFunctions are made with the keyword function(), can have one or more arguments separated by commas, and need assigning to a name.\nFor a function that multiplies two numbers, we need two arguments, arg1 and arg2. We will give the function a name. Itâ€™s useful to give the function a name that has a meaning and not to use my_function.\n\nmy_multiplier <- function(arg1, arg2){\n  arg1 * arg2\n}\n\nTo run the function, we type the name of the function and define the values for each argument. The function will then return the result.\n\nmy_multiplier(arg1 = 3, arg2 = 4)\n\n[1] 12\n\n\nWe can also make more complicated functions, for example one that runs a linear regression. The function has one argument, which is the data. The function contains a linear regression with value as response and Treatment as predictor. We name the function fit_model.\nTo run the function we type the name of the function and define which data that should be used.\n\nfit_model <- function(data){\n  mod <- lm(Value ~ Treatment, data = data)\n  mod\n}\n\nfit_model(data = my_data)"
  },
  {
    "objectID": "02-abstraction.html#functions",
    "href": "02-abstraction.html#functions",
    "title": "2Â  Abstraction",
    "section": "2.3 Functions",
    "text": "2.3 Functions\nA function is a self contained modules of code that accomplish a specific task. R has many built-in functions that we use all the time. For example mean() which calculates the arithmetic mean.\nWe can also make our own functions and use them in our code. Functions are useful when a task is done several times. It saves repetition and makes the code more compact and clear. Functions can be called several times.\nLetâ€™s make our own function.\n\n2.3.1 Make custom functions\nWe want to multiplies two numbers, but the numbers are not always the same. This is a case to use a function.\nFunctions are made with the keyword function(), can have one or more arguments separated by commas, and need assigning to a name.\nFor a function that multiplies two numbers, we need two arguments, arg1 and arg2. We will give the function a name. Itâ€™s useful to give the function a name that has a meaning and not to use my_function.\n\nmy_multiplier <- function(arg1, arg2){\n  arg1 * arg2\n}\n\nTo run the function, type the name of the function and define values for each argument. The function will then return the result.\n\nmy_multiplier(arg1 = 3, arg2 = 4)\n\n[1] 12\n\n\nA more complicated functions, would be to run a linear regression. The function has three arguments: data, response and predictor. It contains the model for a linear regression where the user can choose the data, the response and predictor. The name of the function is fit_model.\nTo run the function, type the name of the function and define all arguments. The output of the function can be stored as an object.\n\nfit_model <- function(data, response, predictor){\n  mod <- lm(response ~ predictor, data = data)\n  mod\n}\n\nmy_model <- fit_model(data = my_data, response = Value, predictor = Treatment)\nmy_model"
  },
  {
    "objectID": "02-abstraction.html#resources",
    "href": "02-abstraction.html#resources",
    "title": "2Â  Abstraction",
    "section": "2.4 Resources",
    "text": "2.4 Resources\n\nFilazzola, A., & Lortie, C. J. (2022). A call for clean code to effectively communicate science. Methods in Ecology and Evolution, 13(10), 2119-2128."
  },
  {
    "objectID": "02-abstraction.html#how-to-deal-with-complex-code",
    "href": "02-abstraction.html#how-to-deal-with-complex-code",
    "title": "2Â  Abstraction",
    "section": "2.1 How to deal with complex code",
    "text": "2.1 How to deal with complex code\nTo understand why targets is useful, we need to introduce the concept of abstraction.\nIn biology, code is used to manage data, conduct data analysis and visualization, and to some extent writing reports and making presentations.\nCode should be clear and communicate the purpose of the code well. It should be transparent and reproducible, which increases the chance that it is understood by others and by your later self.\nCode in data analysis is however often long and complex, with several different pieces of code for different tasks. This quickly becomes messy and unclear.\nAn example for such code would be:\n\nimport data\nclean the data\nperform an analysis\nmake a figure to visualize the result.\n\nThis workflow requires a bunch of different pieces of code that is run sequentially. Managing and keeping track of all the code is complex and quickly becomes messy. There is a better approach called abstraction."
  },
  {
    "objectID": "02-abstraction.html#the-concept-of-abstraction",
    "href": "02-abstraction.html#the-concept-of-abstraction",
    "title": "2Â  Abstraction",
    "section": "2.2 The concept of abstraction",
    "text": "2.2 The concept of abstraction\nâ€œAbstraction should break down complex code chunks into smaller, self-explanatory tasks to better describe the purpose or the scriptâ€ (Filazzola & Lortie, 2022).\nIn principle, the code is split up in different parts and levels. The main script at the highest level contains the main steps of the workflow, but the details and complexity is removed. When reading the main script, one can understand what is going on, but does not need to go into the details. The rest of the code is sourced from the main script and hidden at a lower level. All the details are available.\nCode that has similar function should be grouped together. For example, code to clean the data can be merged into one function.\nLetâ€™s make an example.\nHere is code that imports and cleans data, runs a model and produces a figure. Note that we are using a small example here to save space, but this code would likely be more complex in reality.\n\n### Script to test how warming affects plant height in Bistorta vivipara.\n\n# load libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(performance)\n\n# import data\nraw_traits <- read_delim(file = here(\"data/PFTC4_Svalbard_2018_ITEX_Traits.csv\")) \n\n# data cleaning\ntraits <- raw_traits |> \n  # remove NAs\n  filter(!is.na(Value)) |> \n  # fix typo in taxon\n  mutate(Taxon = if_else(Taxon == \"bistorta vivpara\", \"bistorta vivipara\")) |> \n  # order factor and rename variable gradient\n  mutate(Gradient = case_match(\"C\" ~ \"Control\",\n                               \"B\" ~ \"Nutrients\"),\n         Gradient = factor(Gradient, levels = c(\"Control\", \"Nutrients\")))\n\n# prepare data for analysis and filter bistorta species and plant height\nbistorta <- traits |>\n  filter(Taxon == \"bistorta vivipara\",\n         Trait == \"Plant_Height_cm\")\n\n# run a linear model\nmod_height <- lm(Value ~ Treatment, data = bistorta)\nsummary(mod_height)\n# check model assumptions\ncheck_model(mod_height)\n\n# plot treatments vs plant height\nggplot(bistorta, aes(x = Treatment, y = Value)) +\n  geom_boxplot(fill = c(\"grey80\", \"red\")) +\n  labs(x = \"Treatment\", y = \"Plant height (cm)\")\n\nThis is a long script and the code has to be studied very carefully to understand all the steps.\nWith the concept of abstraction, the code would be split into a main script and most of the code would be moved to scripts at the lower level.\nLetâ€™s look at the main script:\n\n### Script to test how warming affects plant height in Bistorta vivipara.\n\n# load libraries\nsource(load_libraries.R)\n\n# import and clean data\ntraits <- importa_and_clean_traits(path = \"data/PFTC4_Svalbard_2018_ITEX_Traits.csv\")\n\n# run a linear model for bistorta and plant height\nmod_height <- fit_model(data = traits |>\n  filter(Taxon == \"bistorta vivipara\",\n         Trait == \"Plant_Height_cm\"))\nsummary(mod_height)\n# check model assumptions\ncheck_model(mod_height)\n\n# plot treatments vs. plant height\nfig_height <- make_figure(bistorta)\n\nThe rest of the code can be hidden in two scripts. One that loads all the libraries.\n\n# load libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(performance)\n\nA second script that contains three functions to import and clean the data, run a model, and make a figure.\n\n### My custom functions\n\n# import and clean data\nimporta_and_clean_traits <- function(path){\n  traits <- read_delim(file = here(path)) |> \n  # remove NAs\n  filter(!is.na(Value)) |> \n  # fix typo in taxon\n  mutate(Taxon = if_else(Taxon == \"bistorta vivpara\", \"bistorta vivipara\")) |> \n  # order factor and rename variable gradient\n  mutate(Gradient = case_match(\"C\" ~ \"Control\",\n                               \"B\" ~ \"Nutrients\"),\n         Gradient = factor(Gradient, levels = c(\"Control\", \"Nutrients\")))\n}\n\n# run a linear model\nfit_model <- function(data){\n  mod <- lm(Value ~ Treatment, data = data)\n  mod\n}\n\n# plot treatments vs plant height\nmake_figure <- function(bistorta){\n  ggplot(bistorta, aes(x = Treatment, y = Value)) +\n  geom_boxplot(fill = c(\"grey80\", \"red\")) +\n  labs(x = \"Treatment\", y = \"Plant height (cm)\")\n}\n\nUsing abstraction in coding needs practice and there is often not one right but multiple solutions. A good way to start is to make a plan for what parts of code chunks there will be and what they will do. Then the code can be organize into main and other parts. Also, decide when to use functions, for example for code that is used repetitively.\nWe have talked a lot about functions. In the next part we will explain what a function is and how to make one."
  }
]